<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="DiffMatch: Diffusion Model for Dense Matching">
  <meta name="keywords" content="Diffusion, Matching">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>DiffMatch: Diffusion Model for Dense Matching</title>
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">DiffMatch: Diffusion Model for Dense Matching</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://scholar.google.com/citations?hl=ko&user=xakYe8MAAAAJ">Jisu Nam</a>,</span>
            <span class="author-block">
              Gyuseong Lee</a>,</span>
            <span class="author-block">
              <a href="https://github.com/sunwoo76">Sunwoo Kim</a>,
            </span>
            <span class="author-block">
              <a href="https://ines-hyeonsu-kim.github.io">Hyeonsu Kim</a>,
            </span>
            <span class="author-block">
              Hyoungwon Cho</a>,
            </span>
            <span class="author-block">
              Seyeon Kim</a>,
            </span>
            <span class="author-block">
              <a href="https://cvlab.korea.ac.kr/members/faculty">Seungryong Kim</a>
            </span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block">Korea University</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="https://arxiv.org/list/cs/new"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            
              <span class="link-block">
                <a href="https://github.com/KU-CVLAB/DiffMatch"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

 <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
        <div style="display: flex; justify-content: center;">
            <div style="flex: 1; display: flex; flex-direction: column; align-items: center; margin-right: 3px;">
                <img src="static/images/src_1.png" alt="Source Image" style="width:100%;"/>
                <img src="static/images/src_2.png" alt="Source Image" style="width:100%;"/>
            </div>
            <div style="flex: 1; display: flex; flex-direction: column; align-items: center; margin-right: 3px;">
                <img src="static/images/trg_1.png" alt="Target Image" style="width:100%;"/>
                <img src="static/images/trg_2.png" alt="Target Image" style="width:100%;"/>
            </div>
            <div style="flex: 1; display: flex; flex-direction: column; align-items: center; margin-right: 3px;">
                <img src="static/images/vis_1.gif" alt="Diff Match Image" style="width:100%;"/>
                <img src="static/images/vis_2.gif" alt="Diff Match Image" style="width:100%;"/>
            </div>
            <div style="flex: 1; display: flex; flex-direction: column; align-items: center;">
                <img src="static/images/warped_1.png" alt="GT Image" style="width:100%;"/>
                <img src="static/images/warped_2.png" alt="GT Image" style="width:100%;"/>
            </div>
        </div>
        <div style="display: flex; justify-content: center;">
            <div style="flex: 1; display: flex; justify-content: center; margin-right: 3px;">
                <span style="text-align: center;">Source</span>
            </div>
            <div style="flex: 1; display: flex; justify-content: center; margin-right: 3px;">
                <span style="text-align: center;">Target</span>
            </div>
            <div style="flex: 1; display: flex; justify-content: center; margin-right: 3px;">
                <span style="text-align: center;">DiffMatch</span>
            </div>
            <div style="flex: 1; display: flex; justify-content: center;">
                <span style="text-align: center;">GT</span>
            </div>
        </div>
        <div class="content has-text-justified">
            <p style="font-size: 15px;">
              <strong>Visualization of the reverse diffusion process for dense correspondence:</strong> (from left to right) source and target images, warped source images by estimated correspondences as evolving time steps, and ground-truth. The source image is progressively warped into the target image through an iterative denoising process.
            </p>
        </div>
    </div>

    <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <img width="100%" src="static/images/teaser.png"> <br>
          <div style="display: flex; justify-content: center; text-align: center; margin-bottom: 5px;"> 
          <div class="content has-text-justified">
            <p style="font-size: 15px;">
              <strong>Visualizing the effectiveness of the proposed DiffMatch:</strong> (a) source images, (b) target images, and warped source images using estimated correspondences by (c-d) state-of-the-art approaches, (e) our DiffMatch, and (f) ground-truth. Compared to previous methods that discriminatively estimate correspondences, our diffusion-based generative framework effectively learns the matching field manifold, resulting in better estimating correspondences particularly at textureless regions, repetitive patterns, and large displacements.
            </p>
          </div>
        </div>
      </div>
    </div>  
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p style="font-size: 15px;">
            The objective for establishing dense correspondence between paired images consists of two terms: a data term and a prior term. 
            While conventional techniques focused on defining hand-designed prior terms, which are difficult to formulate, recent approaches 
            have focused on learning the data term with deep neural networks without explicitly modeling the prior, assuming that the model 
            itself has the capacity to learn an optimal prior from a large-scale dataset. The performance improvement was obvious, however, 
            they often fail to address inherent ambiguities of matching, such as textureless regions, repetitive patterns, and large displacements. 
            To address this, we propose <i>DiffMatch</i>, a novel conditional diffusion-based framework designed to explicitly model both the data and prior terms. 
            Unlike previous approaches, this is accomplished by leveraging a conditional denoising diffusion model. 
            <i>DiffMatch</i> consists of two main components: conditional denoising diffusion module and cost injection module. 
            We stabilize the training process and reduce memory usage with a stage-wise training strategy. 
            Furthermore, to boost performance, we introduce an inference technique that finds a better path to the accurate matching field. 
            Our experimental results demonstrate significant performance improvements of our method over existing approaches, 
            and the ablation studies validate our design choices along with the effectiveness of each component.
          </p>
        </div>
      </div>
    </div>
    <br>
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Framework</h2>
          <img width="100%" src="static/images/diffmatch_main-1.png"> <br>
          <div style="display: flex; justify-content: center; text-align: center; margin-bottom: 5px;">
            
          <div class="content has-text-justified">
            <p style="font-size: 15px;">
              <strong>Overall network architecture of DiffMatch.</strong> Our network consists of two key components: a conditional denoising diffusion module that generates the matching fields based on the conditions, and a cost injection module that embeds further pixel-wise interactions among paired source and target images into the diffusion module.
            </p>
          </div>
        </div>
      </div>
    </div> 
    <br>

    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Inference</h2>
          <img style="max-width: 100%;" src="static/images/diffmatch_inference-1.png">
          <div class="content has-text-justified">
            <p style="font-size: 15px;">
              <strong>Illustration of the proposed inference technique:</strong> To stabilize the reverse diffusion process, we measure the mean of estimated multiple matching flows from different initializations. We iteratively reinitialize the initial matching field with the previously estimated one. Subsequently, we find the most confident estimated match among the flow candidates through cycle consistency.
            </p>
          </div>
        </div>
      </div>
    </div>
    <br>
    <br>
    
    
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Matching Results</h2>
          <img style="max-width: 100%;" src="static/images/hpatches.png">
          <div class="content has-text-justified">
            <p style="font-size: 15px;">
              <strong>Qualitative results on HPatches:</strong> the source images are warped to the target images using predicted correspondences.
            </p>
          </div>
       
          <img style="max-width: 100%;" src="static/images/eth3d.png">
          <div class="content has-text-justified">
            <p style="font-size: 15px;">
              <strong>Qualitative results on ETH3D:</strong> the source images are warped to the target images using predicted correspondences.
            </p>
          </div>

          <img style="max-width: 100%;" src="static/images/hp_perturb_final-1.png">
          <div class="content has-text-justified">
            <p style="font-size: 15px;">
              <strong>Qualitative results on HPatches using corruptions in ImageNet-C:</strong> the source images are warped to the target images using predicted correspondences.
            </p>
          </div>

          <img style="max-width: 100%;" src="static/images/eth3d_perturb-1.png">
          <div class="content has-text-justified">
            <p style="font-size: 15px;">
              <strong>Qualitative results on ETH3D using corruptions in ImageNet-C:</strong> the source images are warped to the target images using predicted correspondences.
                 
            </p>
          </div>
        </div>
      </div>
    </div>
    <br>
    <br>
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{nam2023diffmatch,
  author    = {TBD},
  title     = {TBD},
  journal   = {TBD},
  year      = {2023},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="https://arxiv.org/list/cs/new">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/KU-CVLAB/DiffMatch" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://ku-cvlab.github.io/DiffMatch/">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
